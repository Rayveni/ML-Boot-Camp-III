{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "from project import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y = utils.load_ensemble_data(flip_columns=False, extended_columns=False, tsne=False, kmeans=True)\n",
    "X1, _, _ = utils.load_data('flipped2')\n",
    "X_train = pd.concat((X_train, X1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bonus_columns = X_train\n",
    "prev_best = [\n",
    "       '04_KNN_POWER_col_2', '04_KNN_POWER_col_3', '04_KNN_POWER_col_5',\n",
    "       '04_KNN_POWER_col_6', '01_XGB_split', '08_NN_flip', '11_LGB_split',\n",
    "       '02_XGB_flip', 'doReturnOnLowerLevels', 'last_lvl_completed',\n",
    "       'clean_start',\n",
    "    \n",
    "       'totalScore2', 'totalNumOfAttempts',\n",
    "       'numberOfBoostersUsed', 'f_16_inv',\n",
    "       'averageNumOfTurnsPerCompletedLevel', 'f_old_7', \n",
    "    #'f_3',\n",
    "       'totalBonusScore2', 'totalStarsCount2'\n",
    "    \n",
    "]\n",
    "X_train = pd.DataFrame()\n",
    "for c in prev_best:\n",
    "    X_train = pd.concat((X_train, bonus_columns[c]), axis=1)\n",
    "    bonus_columns.drop(c, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "\n",
    "    'colsample_bytree': 0.8,\n",
    "    'gamma': 0.05,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 3,\n",
    "    'min_child_weight': 2,\n",
    "    'n_estimators': 700,\n",
    "    'reg_alpha': 0,\n",
    "    'reg_lambda': 6.0,\n",
    "    'subsample': 0.8,\n",
    "    \n",
    "#  'colsample_bytree': 0.8,\n",
    "#  'gamma': 0.05,\n",
    "#  'learning_rate': 0.01,\n",
    "#  'max_depth': 3,\n",
    "#  'min_child_weight': 2,\n",
    "#  'n_estimators': 569+70,\n",
    "#  'reg_alpha': 0.3,\n",
    "#  'reg_lambda': 0.42,\n",
    "#  'subsample': 0.65,\n",
    "\n",
    "    'nthread': 4,\n",
    "    'seed': 1502,\n",
    "    'silent': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if(prev_best):\n",
    "    model = xgb.XGBClassifier(**PARAMS)\n",
    "    par = PARAMS.copy()\n",
    "    par['num_leaves'] = 2 ** par['max_depth']\n",
    "    del par['gamma']\n",
    "    del par['max_depth']\n",
    "    model = lgb.LGBMClassifier(**par)\n",
    "    scores = cross_val_score(model, X_train, Y, cv=20, scoring='neg_log_loss', fit_params={'eval_metric':'logloss'})\n",
    "    print(np.mean(scores), scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "from xgboost import plot_importance\n",
    "if(prev_best):    \n",
    "    model = xgb.XGBClassifier(**PARAMS)\n",
    "    model.fit(X_train, Y, eval_metric='logloss')\n",
    "    for a, b in sorted(zip(model.feature_importances_, X_train.columns)):\n",
    "        print(a,b, sep='\\t\\t')\n",
    "    plot_importance(model)\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CLASSIFIER = 'xgb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=5, random_state=13, shuffle=True)\n",
    "for i in range(15):\n",
    "    best_score = 1\n",
    "    best_column = ''\n",
    "    best_std = 1\n",
    "    \n",
    "    worst_score = 0\n",
    "    worst_column = ''\n",
    "    for column in bonus_columns:\n",
    "        temp_X_train = pd.concat((X_train, bonus_columns[column]), axis=1)\n",
    "        if CLASSIFIER == 'xgb':\n",
    "            model = xgb.XGBClassifier(**PARAMS)\n",
    "        else:\n",
    "            par = PARAMS.copy()\n",
    "            par['num_leaves'] = 2 ** par['max_depth']\n",
    "            del par['gamma']\n",
    "            del par['max_depth']\n",
    "            model = lgb.LGBMClassifier(**par)\n",
    "        scores = cross_val_score(model,\n",
    "                                         temp_X_train,\n",
    "                                         Y,\n",
    "                                         cv=kf,\n",
    "                                         scoring='neg_log_loss',\n",
    "                                         fit_params={'eval_metric':'logloss'}\n",
    "                                )\n",
    "        score = round(-np.mean(scores), 6)\n",
    "        std = np.std(scores)\n",
    "        print(column, ':\\t', score,'\\t', std, sep='')\n",
    "        #best_results.append((score, column))\n",
    "        if (score < best_score) or (score == best_score and std < best_std):\n",
    "            best_score = score\n",
    "            best_column = column\n",
    "            best_std = std\n",
    "        elif score > worst_score:\n",
    "            worst_score = score\n",
    "            worst_column = column\n",
    "\n",
    "\n",
    "    X_train = pd.concat((X_train, bonus_columns[best_column]), axis=1)\n",
    "    print('Iter', i)\n",
    "    print('Best columns:', X_train.columns, 'with score:', best_score)\n",
    "    print('Worst column:', worst_column, 'with score:', worst_score)\n",
    "    #log_bot('Iter '+ str(i) +' Worst column ' + str(worst_column) + ' with score ' + str(worst_score))\n",
    "    #log_bot('Iter '+ str(i) +'\\r\\n' + str(X_train.columns) + '\\r\\nScore: ' + str(best_score) + ' Std: ' + str(best_std))\n",
    "    bonus_columns.drop(best_column, axis=1, inplace=True)\n",
    "    #bonus_columns.drop(worst_column, axis=1, inplace=True)\n",
    "print('Final features:', X_train.columns)\n",
    "#log_bot('Final features :' + str(X_train.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Feature drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=10, random_state=13, shuffle=True)\n",
    "for i in range(5):\n",
    "    best_score = 1\n",
    "    best_column = ''\n",
    "    best_std = 1\n",
    "    \n",
    "    for column in X_train.columns:\n",
    "        if CLASSIFIER == 'xgb':\n",
    "            model = xgb.XGBClassifier(**PARAMS)\n",
    "        else:\n",
    "            par = PARAMS.copy()\n",
    "            par['num_leaves'] = 2 ** par['max_depth']\n",
    "            del par['gamma']\n",
    "            del par['max_depth']\n",
    "            model = lgb.LGBMClassifier(**par)\n",
    "        temp_X_train = X_train.drop(column, axis=1)\n",
    "        scores = cross_val_score(model,\n",
    "                                         temp_X_train,\n",
    "                                         Y,\n",
    "                                         cv=kf,\n",
    "                                         scoring='neg_log_loss',\n",
    "                                         fit_params={'eval_metric':'logloss'}\n",
    "                                )\n",
    "        score = round(-np.mean(scores), 6)\n",
    "        std = np.std(scores)\n",
    "        print(column, ':\\t', score,'\\t', std, sep='')\n",
    "        if score < best_score or (score == best_score and std < best_std):\n",
    "            best_score = score\n",
    "            best_column = column\n",
    "            best_std = std\n",
    "    print('===Best column:', best_column, 'with score:', best_score)\n",
    "    X_train.drop(best_column, axis=1, inplace=True)\n",
    "print('Final features:', X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train.columns\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
